{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac41238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e225f4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'are', 'viruses', 'different', 'from', 'other', 'microbes', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Why are viruses different from other microbes?\"\n",
    "tokens = tokenizer.tokenize(text.lower())\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29aeb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\akqp4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (980, 2)\n",
      "Columns: ['Question', 'Answer']\n",
      "First few rows of original data:\n",
      "                                            Question  \\\n",
      "0      What is the Heisenberg Uncertainty Principle?   \n",
      "1  What is the chemical formula for rust?\",Fe₂O₃ ...   \n",
      "2  What is the name of the galaxy our solar syste...   \n",
      "3                           What causes ocean tides?   \n",
      "4                                What is a catalyst?   \n",
      "\n",
      "                                              Answer  \n",
      "0  It states that the position and momentum of a ...  \n",
      "1  To generate energy (ATP) through cellular resp...  \n",
      "2                              The Milky Way galaxy.  \n",
      "3  Primarily the gravitational pull of the Moon a...  \n",
      "4  A substance that speeds up a chemical reaction...  \n",
      "\n",
      "\n",
      "Using columns: 'Question' and 'Answer'\n",
      "\n",
      "Starting preprocessing...\n",
      "\n",
      "Total rows to process: 980\n",
      "\n",
      "Processing column: Question\n",
      "  Processed 100/980 rows...\n",
      "  Processed 200/980 rows...\n",
      "  Processed 300/980 rows...\n",
      "  Processed 400/980 rows...\n",
      "  Processed 500/980 rows...\n",
      "  Processed 600/980 rows...\n",
      "  Processed 700/980 rows...\n",
      "  Processed 800/980 rows...\n",
      "  Processed 900/980 rows...\n",
      "✓ Completed Question\n",
      "  Success: 980, Errors: 0\n",
      "\n",
      "Processing column: Answer\n",
      "  Processed 100/980 rows...\n",
      "  Processed 200/980 rows...\n",
      "  Processed 300/980 rows...\n",
      "  Processed 400/980 rows...\n",
      "  Processed 500/980 rows...\n",
      "  Processed 600/980 rows...\n",
      "  Processed 700/980 rows...\n",
      "  Processed 800/980 rows...\n",
      "  Processed 900/980 rows...\n",
      "✓ Completed Answer\n",
      "  Success: 980, Errors: 0\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "\n",
      "Output saved to: preprocessed_science_qa.csv\n",
      "Total rows processed: 980\n",
      "Original CSV had: 980 rows\n",
      "\n",
      "First 10 rows of preprocessed data:\n",
      "                                            Question  \\\n",
      "0                   heisenberg uncertainty principle   \n",
      "1  chemical formula rustfeo iron oxide primary fu...   \n",
      "2                    name galaxy solar system locate   \n",
      "3                                   cause ocean tide   \n",
      "4                                           catalyst   \n",
      "5                          primary function ribosome   \n",
      "6                                cause earths season   \n",
      "7                           tectonic plate important   \n",
      "8                                     photosynthesis   \n",
      "9                                         black hole   \n",
      "\n",
      "                                              Answer  \n",
      "0  state position momentum particle precisely kno...  \n",
      "1  generate energy atp cellular respiration often...  \n",
      "2                                   milky way galaxy  \n",
      "3  primarily gravitational pull moon lesser exten...  \n",
      "4  substance speed chemical reaction without cons...  \n",
      "5  ribosome protein synthesis machinery cell tran...  \n",
      "6  earth axial tilt degree cause different part p...  \n",
      "7  tectonic plate large piece earth lithosphere m...  \n",
      "8  photosynthesis process use plant organisms con...  \n",
      "9  black hole region spacetime gravity strong not...  \n",
      "\n",
      "Last 5 rows of preprocessed data:\n",
      "                Question                                             Answer\n",
      "975  saffirsimpson scale  saffirsimpson hurricane wind scale category ra...\n",
      "976         fujita scale  enhance fujita ef scale use rate intensity str...\n",
      "977             pedagogy  pedagogy method practice teach especially acad...\n",
      "978            andragogy  andragogy method practice teach adult learner ...\n",
      "979          gerontology  gerontology comprehensive scientific study age...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources (do this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # New English-specific tagger\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()  # Using tokenizer that does NOT require 'punkt'\n",
    "\n",
    "# Convert POS tags to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Preprocessing function using lemmatization and TreebankWordTokenizer\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # 2. Remove non-alphabetic characters (keep spaces)\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # 3. Tokenize using TreebankWordTokenizer (no punkt required)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # 4. Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # 5. POS tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "\n",
    "        # 6. Lemmatize with POS\n",
    "        lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "        # Return clean, lemmatized text string\n",
    "        return ' '.join(lemmatized)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"science_qa.csv\"  # Update this path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"First few rows of original data:\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check if columns exist (case-insensitive)\n",
    "available_cols = df.columns.tolist()\n",
    "question_col = None\n",
    "answer_col = None\n",
    "\n",
    "for col in available_cols:\n",
    "    if col.lower() == 'question':\n",
    "        question_col = col\n",
    "    elif col.lower() == 'answer':\n",
    "        answer_col = col\n",
    "\n",
    "if not question_col or not answer_col:\n",
    "    print(f\"ERROR: Could not find 'question' and 'answer' columns!\")\n",
    "    print(f\"Available columns: {available_cols}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Using columns: '{question_col}' and '{answer_col}'\\n\")\n",
    "\n",
    "# Keep only Question and Answer columns (skip timestamp)\n",
    "columns_to_process = [question_col, answer_col]\n",
    "df_processed = df[columns_to_process].copy()\n",
    "\n",
    "print(\"Starting preprocessing...\\n\")\n",
    "print(f\"Total rows to process: {len(df_processed)}\\n\")\n",
    "\n",
    "# Track errors\n",
    "error_count = 0\n",
    "success_count = 0\n",
    "\n",
    "# Preprocess each column\n",
    "for col in columns_to_process:\n",
    "    print(f\"Processing column: {col}\")\n",
    "    processed_values = []\n",
    "    \n",
    "    for idx, value in enumerate(df_processed[col]):\n",
    "        try:\n",
    "            result = preprocess_text(value)\n",
    "            processed_values.append(result)\n",
    "            success_count += 1\n",
    "            \n",
    "            # Show progress every 100 rows\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df_processed)} rows...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error at row {idx}: {e}\")\n",
    "            processed_values.append(\"\")\n",
    "            error_count += 1\n",
    "    \n",
    "    df_processed[col] = processed_values\n",
    "    print(f\"✓ Completed {col}\")\n",
    "    print(f\"  Success: {success_count}, Errors: {error_count}\\n\")\n",
    "    \n",
    "    # Reset counters for next column\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "# Save preprocessed dataset\n",
    "output_path = \"preprocessed_science_qa.csv\"\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Preprocessing completed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutput saved to: {output_path}\")\n",
    "print(f\"Total rows processed: {len(df_processed)}\")\n",
    "print(f\"Original CSV had: {len(df)} rows\")\n",
    "print(f\"\\nFirst 10 rows of preprocessed data:\")\n",
    "print(df_processed.head(10))\n",
    "print(f\"\\nLast 5 rows of preprocessed data:\")\n",
    "print(df_processed.tail(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
