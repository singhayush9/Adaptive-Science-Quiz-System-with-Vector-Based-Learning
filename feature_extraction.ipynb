{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8354ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace638ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Detected columns: ['question1', 'question2', 'question3', 'answer1', 'answer2', 'answer3']\n",
      "Found paired indices: [1, 2, 3]\n",
      "\n",
      "Processing pair: question1 <-> answer1\n",
      " - Embedding 4118 texts from 'question1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Embedding 4118 texts from 'answer1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Saved: NLP\\question1_embeddings.npy ((4118, 768)), NLP\\answer1_embeddings.npy ((4118, 768))\n",
      "\n",
      "Processing pair: question2 <-> answer2\n",
      " - Embedding 4118 texts from 'question2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Embedding 4118 texts from 'answer2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Saved: NLP\\question2_embeddings.npy ((4118, 768)), NLP\\answer2_embeddings.npy ((4118, 768))\n",
      "\n",
      "Processing pair: question3 <-> answer3\n",
      " - Embedding 4118 texts from 'question3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Embedding 4118 texts from 'answer3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Saved: NLP\\question3_embeddings.npy ((4118, 768)), NLP\\answer3_embeddings.npy ((4118, 768))\n",
      "\n",
      "Processing complete. Summary:\n",
      "Pair 1: question1 <-> answer1 | shapes: (4118, 768) , (4118, 768)\n",
      "Pair 2: question2 <-> answer2 | shapes: (4118, 768) , (4118, 768)\n",
      "Pair 3: question3 <-> answer3 | shapes: (4118, 768) , (4118, 768)\n",
      "\n",
      "All embeddings saved to: NLP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "file_path = r\"Preprocessed_QA_Dataset.csv\"\n",
    "output_dir = r\"NLP\"\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 32 \n",
    "\n",
    "# Device and model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Utilities\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden)\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def batch_embed_texts(texts, batch_size=batch_size):\n",
    "    \"\"\"Return numpy array of shape (len(texts), hidden_size).\"\"\"\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Batches\", leave=False):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                           padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        emb = mean_pooling(outputs, inputs['attention_mask'])\n",
    "        emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "        all_embs.append(emb.cpu().numpy())\n",
    "    if len(all_embs) == 0:\n",
    "        return np.zeros((0, model.config.hidden_size), dtype=np.float32)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "\n",
    "# Load dataset & detect columns\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "# normalize column names\n",
    "orig_columns = df.columns.tolist()\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "print(\"Detected columns:\", df.columns.tolist())\n",
    "\n",
    "q_pattern = re.compile(r\"^question(\\d+)$\")\n",
    "a_pattern = re.compile(r\"^answer(\\d+)$\")\n",
    "\n",
    "q_cols = {}\n",
    "a_cols = {}\n",
    "for col in df.columns:\n",
    "    m = q_pattern.match(col)\n",
    "    if m:\n",
    "        q_cols[int(m.group(1))] = col\n",
    "    m2 = a_pattern.match(col)\n",
    "    if m2:\n",
    "        a_cols[int(m2.group(1))] = col\n",
    "\n",
    "paired_indices = sorted(set(q_cols.keys()) & set(a_cols.keys()))\n",
    "if not paired_indices:\n",
    "    raise ValueError(\"No matching questionX/answerX column pairs found in the CSV.\")\n",
    "\n",
    "print(\"Found paired indices:\", paired_indices)\n",
    "\n",
    "\n",
    "# Process each pair\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "summary = []\n",
    "\n",
    "for idx in paired_indices:\n",
    "    q_col = q_cols[idx]\n",
    "    a_col = a_cols[idx]\n",
    "    print(f\"\\nProcessing pair: {q_col} <-> {a_col}\")\n",
    "\n",
    "    q_texts = df[q_col].fillna(\"\").astype(str).tolist()\n",
    "    a_texts = df[a_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Embed questions\n",
    "    print(f\" - Embedding {len(q_texts)} texts from '{q_col}'\")\n",
    "    q_embs = batch_embed_texts(q_texts, batch_size=batch_size)\n",
    "\n",
    "    # Embed answers\n",
    "    print(f\" - Embedding {len(a_texts)} texts from '{a_col}'\")\n",
    "    a_embs = batch_embed_texts(a_texts, batch_size=batch_size)\n",
    "\n",
    "    # Save per-pair files\n",
    "    q_npy = os.path.join(output_dir, f\"{q_col}_embeddings.npy\")\n",
    "    a_npy = os.path.join(output_dir, f\"{a_col}_embeddings.npy\")\n",
    "    np.save(q_npy, q_embs)\n",
    "    np.save(a_npy, a_embs)\n",
    "\n",
    "    pd.DataFrame(q_embs).to_csv(os.path.join(output_dir, f\"{q_col}_embeddings.csv\"), index=False)\n",
    "    pd.DataFrame(a_embs).to_csv(os.path.join(output_dir, f\"{a_col}_embeddings.csv\"), index=False)\n",
    "\n",
    "    summary.append((idx, q_col, a_col, q_embs.shape, a_embs.shape))\n",
    "    print(f\" -> Saved: {q_npy} ({q_embs.shape}), {a_npy} ({a_embs.shape})\")\n",
    "\n",
    "\n",
    "print(\"\\nProcessing complete. Summary:\")\n",
    "for s in summary:\n",
    "    print(f\"Pair {s[0]}: {s[1]} <-> {s[2]} | shapes: {s[3]} , {s[4]}\")\n",
    "\n",
    "print(\"\\nAll embeddings saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea218129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCIBERT EMBEDDING GENERATION & CHROMADB STORAGE\n",
      "======================================================================\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Loading SciBERT model...\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "Initializing ChromaDB...\n",
      "‚úì Connected to ChromaDB at: vector_db\n",
      "\n",
      "Creating/loading collections...\n",
      "‚úì Answers collection: 0 existing documents\n",
      "‚úì Questions collection: 0 existing documents\n",
      "\n",
      "======================================================================\n",
      "LOADING PREPROCESSED DATASET\n",
      "======================================================================\n",
      "\n",
      "Dataset loaded: 808 rows\n",
      "Columns: ['Question', 'Answer']\n",
      "\n",
      "Using columns:\n",
      "  Questions: 'Question'\n",
      "  Answers: 'Answer'\n",
      "\n",
      "Total rows: 808\n",
      "Valid entries (non-empty Q&A): 808\n",
      "\n",
      "======================================================================\n",
      "EMBEDDING QUESTIONS\n",
      "======================================================================\n",
      "\n",
      "Embedding 808 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated embeddings: (808, 768)\n",
      "\n",
      "======================================================================\n",
      "EMBEDDING ANSWERS\n",
      "======================================================================\n",
      "\n",
      "Embedding 808 answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated embeddings: (808, 768)\n",
      "\n",
      "======================================================================\n",
      "STORING ANSWERS IN CHROMADB\n",
      "======================================================================\n",
      "\n",
      "Adding 808 new answer documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answer Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stored 808 answers\n",
      "\n",
      "======================================================================\n",
      "STORING QUESTIONS IN CHROMADB\n",
      "======================================================================\n",
      "\n",
      "Adding 808 new question documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stored 808 questions\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Processing Complete:\n",
      "   Valid Q&A pairs processed: 808\n",
      "   New answers stored: 808\n",
      "   New questions stored: 808\n",
      "\n",
      "üìä ChromaDB Collections:\n",
      "   Answers collection: 808 total documents\n",
      "   Questions collection: 808 total documents\n",
      "\n",
      "üíæ Database location: vector_db\n",
      "\n",
      "======================================================================\n",
      "TESTING DATABASE\n",
      "======================================================================\n",
      "\n",
      "Performing test query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 326\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# Get first question\u001b[39;00m\n\u001b[0;32m    321\u001b[0m first_question_data \u001b[38;5;241m=\u001b[39m questions_collection\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    322\u001b[0m     limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    323\u001b[0m     include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    324\u001b[0m )\n\u001b[1;32m--> 326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_question_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    327\u001b[0m     test_question \u001b[38;5;241m=\u001b[39m first_question_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    328\u001b[0m     test_embedding \u001b[38;5;241m=\u001b[39m first_question_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "csv_file = \"preprocessed_science_qa.csv\"  # Use preprocessed dataset\n",
    "vector_db_dir = \"vector_db\"\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 32\n",
    "\n",
    "# Create vector DB directory\n",
    "os.makedirs(vector_db_dir, exist_ok=True)\n",
    "\n",
    "# ==================== INITIALIZE SCIBERT ====================\n",
    "print(\"=\"*70)\n",
    "print(\"SCIBERT EMBEDDING GENERATION & CHROMADB STORAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "print(\"\\nLoading SciBERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"‚úì Model loaded successfully\")\n",
    "\n",
    "# ==================== EMBEDDING UTILITIES ====================\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pooling over token embeddings.\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def batch_embed_texts(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Embed a list of texts using SciBERT in batches.\n",
    "    Returns: numpy array of shape (len(texts), 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\", leave=False):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "        \n",
    "        # Normalize\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    if len(all_embeddings) == 0:\n",
    "        return np.zeros((0, model.config.hidden_size), dtype=np.float32)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ==================== INITIALIZE CHROMADB ====================\n",
    "print(\"\\nInitializing ChromaDB...\")\n",
    "\n",
    "try:\n",
    "    settings = Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True,\n",
    "        is_persistent=True\n",
    "    )\n",
    "    client = chromadb.PersistentClient(path=vector_db_dir, settings=settings)\n",
    "    print(f\"‚úì Connected to ChromaDB at: {vector_db_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error connecting to ChromaDB: {e}\")\n",
    "    print(\"Attempting to fix lock issues...\")\n",
    "    \n",
    "    # Try to remove lock files\n",
    "    import glob\n",
    "    lock_files = glob.glob(os.path.join(vector_db_dir, \"*.lock\"))\n",
    "    for lock_file in lock_files:\n",
    "        try:\n",
    "            os.remove(lock_file)\n",
    "            print(f\"  Removed: {lock_file}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try again\n",
    "    client = chromadb.PersistentClient(path=vector_db_dir, settings=settings)\n",
    "    print(\"‚úì Connected successfully after cleanup\")\n",
    "\n",
    "# Create collections\n",
    "print(\"\\nCreating/loading collections...\")\n",
    "\n",
    "# Collection for answers (with embeddings)\n",
    "answers_collection = client.get_or_create_collection(\n",
    "    name=\"science_qa_answers\",\n",
    "    metadata={\"description\": \"Answer embeddings from science Q&A dataset\"}\n",
    ")\n",
    "print(f\"‚úì Answers collection: {answers_collection.count()} existing documents\")\n",
    "\n",
    "# Collection for questions (for topic clustering and retrieval)\n",
    "questions_collection = client.get_or_create_collection(\n",
    "    name=\"science_qa_questions\",\n",
    "    metadata={\"description\": \"Question embeddings for topic clustering\"}\n",
    ")\n",
    "print(f\"‚úì Questions collection: {questions_collection.count()} existing documents\")\n",
    "\n",
    "# ==================== LOAD DATASET ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING PREPROCESSED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nDataset loaded: {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Detect question and answer columns (case-insensitive)\n",
    "question_col = None\n",
    "answer_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if col.lower() == 'question':\n",
    "        question_col = col\n",
    "    elif col.lower() == 'answer':\n",
    "        answer_col = col\n",
    "\n",
    "if not question_col or not answer_col:\n",
    "    raise ValueError(f\"Could not find 'question' and 'answer' columns! Found: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nUsing columns:\")\n",
    "print(f\"  Questions: '{question_col}'\")\n",
    "print(f\"  Answers: '{answer_col}'\")\n",
    "\n",
    "# Get data\n",
    "questions = df[question_col].fillna(\"\").astype(str).tolist()\n",
    "answers = df[answer_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Filter out empty entries\n",
    "valid_indices = [\n",
    "    i for i in range(len(answers)) \n",
    "    if answers[i].strip() and questions[i].strip()\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal rows: {len(questions)}\")\n",
    "print(f\"Valid entries (non-empty Q&A): {len(valid_indices)}\")\n",
    "\n",
    "if not valid_indices:\n",
    "    raise ValueError(\"No valid question-answer pairs found!\")\n",
    "\n",
    "# ==================== EMBED QUESTIONS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING QUESTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "valid_questions = [questions[i] for i in valid_indices]\n",
    "print(f\"\\nEmbedding {len(valid_questions)} questions...\")\n",
    "question_embeddings = batch_embed_texts(valid_questions, batch_size=batch_size)\n",
    "print(f\"‚úì Generated embeddings: {question_embeddings.shape}\")\n",
    "\n",
    "# ==================== EMBED ANSWERS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "valid_answers = [answers[i] for i in valid_indices]\n",
    "print(f\"\\nEmbedding {len(valid_answers)} answers...\")\n",
    "answer_embeddings = batch_embed_texts(valid_answers, batch_size=batch_size)\n",
    "print(f\"‚úì Generated embeddings: {answer_embeddings.shape}\")\n",
    "\n",
    "# ==================== STORE ANSWERS IN CHROMADB ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STORING ANSWERS IN CHROMADB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "answer_ids = []\n",
    "answer_documents = []\n",
    "answer_metadatas = []\n",
    "answer_embeddings_list = []\n",
    "\n",
    "for i, orig_idx in enumerate(valid_indices):\n",
    "    doc_id = f\"answer_{orig_idx}\"\n",
    "    \n",
    "    # Check if already exists\n",
    "    try:\n",
    "        existing = answers_collection.get(ids=[doc_id])\n",
    "        if existing['ids']:\n",
    "            continue  # Skip if already exists\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    answer_ids.append(doc_id)\n",
    "    answer_documents.append(valid_answers[i])\n",
    "    answer_metadatas.append({\n",
    "        \"row_index\": int(orig_idx),\n",
    "        \"question\": valid_questions[i],\n",
    "        \"answer_length\": len(valid_answers[i].split())\n",
    "    })\n",
    "    answer_embeddings_list.append(answer_embeddings[i].tolist())\n",
    "\n",
    "# Add to ChromaDB in batches\n",
    "if answer_ids:\n",
    "    batch_size_db = 1000\n",
    "    print(f\"\\nAdding {len(answer_ids)} new answer documents...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(answer_ids), batch_size_db), desc=\"Answer Batches\"):\n",
    "        batch_ids = answer_ids[i:i+batch_size_db]\n",
    "        batch_docs = answer_documents[i:i+batch_size_db]\n",
    "        batch_meta = answer_metadatas[i:i+batch_size_db]\n",
    "        batch_embs = answer_embeddings_list[i:i+batch_size_db]\n",
    "        \n",
    "        try:\n",
    "            answers_collection.add(\n",
    "                ids=batch_ids,\n",
    "                documents=batch_docs,\n",
    "                metadatas=batch_meta,\n",
    "                embeddings=batch_embs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö† Error adding batch: {e}\")\n",
    "    \n",
    "    print(f\"‚úì Stored {len(answer_ids)} answers\")\n",
    "else:\n",
    "    print(\"\\n‚Ñπ All answers already exist in database\")\n",
    "\n",
    "# ==================== STORE QUESTIONS IN CHROMADB ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STORING QUESTIONS IN CHROMADB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "question_ids = []\n",
    "question_documents = []\n",
    "question_metadatas = []\n",
    "question_embeddings_list = []\n",
    "\n",
    "for i, orig_idx in enumerate(valid_indices):\n",
    "    doc_id = f\"question_{orig_idx}\"\n",
    "    \n",
    "    # Check if already exists\n",
    "    try:\n",
    "        existing = questions_collection.get(ids=[doc_id])\n",
    "        if existing['ids']:\n",
    "            continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    question_ids.append(doc_id)\n",
    "    question_documents.append(valid_questions[i])\n",
    "    question_metadatas.append({\n",
    "        \"row_index\": int(orig_idx),\n",
    "        \"answer\": valid_answers[i][:200],  # Store snippet of answer\n",
    "        \"question_length\": len(valid_questions[i].split())\n",
    "    })\n",
    "    question_embeddings_list.append(question_embeddings[i].tolist())\n",
    "\n",
    "# Add to ChromaDB in batches\n",
    "if question_ids:\n",
    "    print(f\"\\nAdding {len(question_ids)} new question documents...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(question_ids), batch_size_db), desc=\"Question Batches\"):\n",
    "        batch_ids = question_ids[i:i+batch_size_db]\n",
    "        batch_docs = question_documents[i:i+batch_size_db]\n",
    "        batch_meta = question_metadatas[i:i+batch_size_db]\n",
    "        batch_embs = question_embeddings_list[i:i+batch_size_db]\n",
    "        \n",
    "        try:\n",
    "            questions_collection.add(\n",
    "                ids=batch_ids,\n",
    "                documents=batch_docs,\n",
    "                metadatas=batch_meta,\n",
    "                embeddings=batch_embs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö† Error adding batch: {e}\")\n",
    "    \n",
    "    print(f\"‚úì Stored {len(question_ids)} questions\")\n",
    "else:\n",
    "    print(\"\\n‚Ñπ All questions already exist in database\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Processing Complete:\")\n",
    "print(f\"   Valid Q&A pairs processed: {len(valid_indices)}\")\n",
    "print(f\"   New answers stored: {len(answer_ids) if answer_ids else 0}\")\n",
    "print(f\"   New questions stored: {len(question_ids) if question_ids else 0}\")\n",
    "\n",
    "print(f\"\\nüìä ChromaDB Collections:\")\n",
    "print(f\"   Answers collection: {answers_collection.count()} total documents\")\n",
    "print(f\"   Questions collection: {questions_collection.count()} total documents\")\n",
    "\n",
    "print(f\"\\nüíæ Database location: {vector_db_dir}\")\n",
    "\n",
    "# ==================== TEST QUERY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING DATABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if answers_collection.count() > 0 and questions_collection.count() > 0:\n",
    "    print(\"\\nPerforming test query...\")\n",
    "    \n",
    "    # Get first question\n",
    "    first_question_data = questions_collection.get(\n",
    "        limit=1,\n",
    "        include=['embeddings', 'documents', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if first_question_data['embeddings']:\n",
    "        test_question = first_question_data['documents'][0]\n",
    "        test_embedding = first_question_data['embeddings'][0]\n",
    "        \n",
    "        print(f\"\\nTest question: {test_question[:100]}...\")\n",
    "        \n",
    "        # Query for similar answers\n",
    "        results = answers_collection.query(\n",
    "            query_embeddings=[test_embedding],\n",
    "            n_results=3,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTop 3 most similar answers:\")\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity = 1 - (distance ** 2) / 2\n",
    "            print(f\"\\n{i+1}. Similarity: {similarity:.2%}\")\n",
    "            print(f\"   Answer: {doc[:100]}...\")\n",
    "            print(f\"   From Row: {metadata['row_index']}\")\n",
    "        \n",
    "        print(\"\\n‚úì Database test successful!\")\n",
    "    else:\n",
    "        print(\"‚ö† No embeddings found for testing\")\n",
    "else:\n",
    "    print(\"‚ö† Database is empty, cannot perform test query\")\n",
    "\n",
    "# ==================== USAGE INSTRUCTIONS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"USAGE INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "The database is now ready! You can use it with:\n",
    "\n",
    "1. Load the collections:\n",
    "   import chromadb\n",
    "   client = chromadb.PersistentClient(path=\"vector_db\")\n",
    "   answers_collection = client.get_collection(\"science_qa_answers\")\n",
    "   questions_collection = client.get_collection(\"science_qa_questions\")\n",
    "\n",
    "2. Query for similar answers (given a question embedding):\n",
    "   results = answers_collection.query(\n",
    "       query_embeddings=[your_question_embedding],\n",
    "       n_results=5\n",
    "   )\n",
    "\n",
    "3. Find similar questions (for topic clustering):\n",
    "   similar_questions = questions_collection.query(\n",
    "       query_embeddings=[current_question_embedding],\n",
    "       n_results=10\n",
    "   )\n",
    "\n",
    "4. Get specific document by ID:\n",
    "   doc = answers_collection.get(ids=[\"answer_42\"])\n",
    "\n",
    "5. Search by text (ChromaDB will embed it for you):\n",
    "   results = answers_collection.query(\n",
    "       query_texts=[\"What is photosynthesis?\"],\n",
    "       n_results=5\n",
    "   )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì‚úì‚úì COMPLETE ‚úì‚úì‚úì\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCIBERT EMBEDDING GENERATION & CHROMADB STORAGE (FIXED)\n",
      "======================================================================\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Loading SciBERT model...\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "Initializing ChromaDB...\n",
      "‚úì Connected to ChromaDB at: vector_db\n",
      "\n",
      "üóëÔ∏è  Deleting old collections...\n",
      "  Deleted: science_qa_answers\n",
      "  Deleted: science_qa_questions\n",
      "\n",
      "Creating new collections...\n",
      "‚úì Answers collection created\n",
      "‚úì Questions collection created\n",
      "\n",
      "======================================================================\n",
      "LOADING PREPROCESSED DATASET\n",
      "======================================================================\n",
      "\n",
      "Dataset loaded: 980 rows\n",
      "Columns: ['Question', 'Answer']\n",
      "\n",
      "Using columns:\n",
      "  Questions: 'Question'\n",
      "  Answers: 'Answer'\n",
      "\n",
      "Total rows: 980\n",
      "Valid entries (non-empty Q&A): 980\n",
      "\n",
      "======================================================================\n",
      "EMBEDDING QUESTIONS\n",
      "======================================================================\n",
      "\n",
      "Embedding 980 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated embeddings: (980, 768)\n",
      "\n",
      "======================================================================\n",
      "EMBEDDING ANSWERS\n",
      "======================================================================\n",
      "\n",
      "Embedding 980 answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated embeddings: (980, 768)\n",
      "\n",
      "======================================================================\n",
      "STORING Q&A PAIRS IN CHROMADB (FIXED)\n",
      "======================================================================\n",
      "\n",
      "üîß KEY FIX: Answers will be indexed by QUESTION embeddings!\n",
      "   This ensures when you query with a question, you get the paired answer.\n",
      "\n",
      "\n",
      "Adding 980 answers (indexed by question embeddings)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answer Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stored 980 answers\n",
      "\n",
      "Adding 980 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Stored 980 questions\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Testing Q&A matching with first 3 pairs...\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Test 1: Row 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Question: heisenberg uncertainty principle...\n",
      "Answer: state position momentum particle precisely know simultaneously accurately one kn...\n",
      "\n",
      "Matched row_index: 0\n",
      "Similarity: 1.0000\n",
      "‚úÖ CORRECT! Matched to same row\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Test 2: Row 1\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Question: chemical formula rustfeo iron oxide primary function mitochondrion cell...\n",
      "Answer: generate energy atp cellular respiration often call powerhouse cell...\n",
      "\n",
      "Matched row_index: 1\n",
      "Similarity: 1.0000\n",
      "‚úÖ CORRECT! Matched to same row\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Test 3: Row 2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Question: name galaxy solar system locate...\n",
      "Answer: milky way galaxy...\n",
      "\n",
      "Matched row_index: 2\n",
      "Similarity: 1.0000\n",
      "‚úÖ CORRECT! Matched to same row\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Processing Complete:\n",
      "   Valid Q&A pairs processed: 980\n",
      "   Answers stored: 980\n",
      "   Questions stored: 980\n",
      "\n",
      "üìä ChromaDB Collections:\n",
      "   Answers collection: 980 documents\n",
      "   Questions collection: 980 documents\n",
      "\n",
      "üíæ Database location: vector_db\n",
      "\n",
      "======================================================================\n",
      "üîß KEY FIX APPLIED:\n",
      "======================================================================\n",
      "\n",
      "BEFORE (Wrong):\n",
      "  - Answers indexed by answer embeddings\n",
      "  - Query with question ‚Üí finds semantically similar answer (WRONG pair!)\n",
      "\n",
      "AFTER (Fixed):\n",
      "  - Answers indexed by QUESTION embeddings\n",
      "  - Query with question ‚Üí finds the PAIRED answer (CORRECT!)\n",
      "\n",
      "This ensures Q&A pairs stay matched correctly!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "USAGE INSTRUCTIONS\n",
      "======================================================================\n",
      "\n",
      "Now when you query:\n",
      "\n",
      "1. Get question embedding:\n",
      "   q_embedding = get_embedding(question_text)\n",
      "\n",
      "2. Query answers collection:\n",
      "   results = answers_collection.query(\n",
      "       query_embeddings=[q_embedding],\n",
      "       n_results=1\n",
      "   )\n",
      "   \n",
      "3. You'll get the PAIRED answer for that question!\n",
      "\n",
      "Note: You can also use the row_index fallback method in your quiz\n",
      "for guaranteed correct matching.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úì‚úì‚úì COMPLETE ‚úì‚úì‚úì\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "csv_file = \"preprocessed_science_qa.csv\"  # Use preprocessed dataset\n",
    "vector_db_dir = \"vector_db\"\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 32\n",
    "\n",
    "# Create vector DB directory\n",
    "os.makedirs(vector_db_dir, exist_ok=True)\n",
    "\n",
    "# ==================== INITIALIZE SCIBERT ====================\n",
    "print(\"=\"*70)\n",
    "print(\"SCIBERT EMBEDDING GENERATION & CHROMADB STORAGE (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "print(\"\\nLoading SciBERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"[SUCCESS] Model loaded successfully\")\n",
    "\n",
    "# ==================== EMBEDDING UTILITIES ====================\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pooling over token embeddings.\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def batch_embed_texts(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Embed a list of texts using SciBERT in batches.\n",
    "    Returns: numpy array of shape (len(texts), 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\", leave=False):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "        \n",
    "        # Normalize\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    if len(all_embeddings) == 0:\n",
    "        return np.zeros((0, model.config.hidden_size), dtype=np.float32)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ==================== INITIALIZE CHROMADB ====================\n",
    "print(\"\\nInitializing ChromaDB...\")\n",
    "\n",
    "try:\n",
    "    settings = Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True,\n",
    "        is_persistent=True\n",
    "    )\n",
    "    client = chromadb.PersistentClient(path=vector_db_dir, settings=settings)\n",
    "    print(f\"[SUCCESS] Connected to ChromaDB at: {vector_db_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Error connecting to ChromaDB: {e}\")\n",
    "    print(\"Attempting to fix lock issues...\")\n",
    "    \n",
    "    # Try to remove lock files\n",
    "    import glob\n",
    "    lock_files = glob.glob(os.path.join(vector_db_dir, \"*.lock\"))\n",
    "    for lock_file in lock_files:\n",
    "        try:\n",
    "            os.remove(lock_file)\n",
    "            print(f\"  Removed: {lock_file}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try again\n",
    "    client = chromadb.PersistentClient(path=vector_db_dir, settings=settings)\n",
    "    print(\"[SUCCESS] Connected successfully after cleanup\")\n",
    "\n",
    "# Delete old collections and create new ones\n",
    "print(\"\\n[DELETE] Deleting old collections...\")\n",
    "try:\n",
    "    client.delete_collection(\"science_qa_answers\")\n",
    "    print(\"  Deleted: science_qa_answers\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    client.delete_collection(\"science_qa_questions\")\n",
    "    print(\"  Deleted: science_qa_questions\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nCreating new collections...\")\n",
    "\n",
    "# Collection for answers (indexed by QUESTION embeddings)\n",
    "answers_collection = client.create_collection(\n",
    "    name=\"science_qa_answers\",\n",
    "    metadata={\"description\": \"Answers indexed by question embeddings (FIXED)\"}\n",
    ")\n",
    "print(f\"[SUCCESS] Answers collection created\")\n",
    "\n",
    "# Collection for questions (for topic clustering and retrieval)\n",
    "questions_collection = client.create_collection(\n",
    "    name=\"science_qa_questions\",\n",
    "    metadata={\"description\": \"Question embeddings for topic clustering\"}\n",
    ")\n",
    "print(f\"[SUCCESS] Questions collection created\")\n",
    "\n",
    "# ==================== LOAD DATASET ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING PREPROCESSED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nDataset loaded: {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Detect question and answer columns (case-insensitive)\n",
    "question_col = None\n",
    "answer_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if col.lower() == 'question':\n",
    "        question_col = col\n",
    "    elif col.lower() == 'answer':\n",
    "        answer_col = col\n",
    "\n",
    "if not question_col or not answer_col:\n",
    "    raise ValueError(f\"Could not find 'question' and 'answer' columns! Found: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nUsing columns:\")\n",
    "print(f\"  Questions: '{question_col}'\")\n",
    "print(f\"  Answers: '{answer_col}'\")\n",
    "\n",
    "# Get data and keep original order\n",
    "questions = df[question_col].fillna(\"\").astype(str).tolist()\n",
    "answers = df[answer_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Filter out empty entries but PRESERVE ORDER\n",
    "valid_pairs = []\n",
    "for i in range(len(questions)):\n",
    "    q = questions[i].strip()\n",
    "    a = answers[i].strip()\n",
    "    if q and a and q.lower() != 'nan' and a.lower() != 'nan':\n",
    "        valid_pairs.append({\n",
    "            'row_index': i,\n",
    "            'question': q,\n",
    "            'answer': a\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal rows: {len(questions)}\")\n",
    "print(f\"Valid entries (non-empty Q&A): {len(valid_pairs)}\")\n",
    "\n",
    "if not valid_pairs:\n",
    "    raise ValueError(\"No valid question-answer pairs found!\")\n",
    "\n",
    "# ==================== EMBED QUESTIONS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING QUESTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "valid_questions = [pair['question'] for pair in valid_pairs]\n",
    "print(f\"\\nEmbedding {len(valid_questions)} questions...\")\n",
    "question_embeddings = batch_embed_texts(valid_questions, batch_size=batch_size)\n",
    "print(f\"[SUCCESS] Generated embeddings: {question_embeddings.shape}\")\n",
    "\n",
    "# ==================== EMBED ANSWERS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING ANSWERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "valid_answers = [pair['answer'] for pair in valid_pairs]\n",
    "print(f\"\\nEmbedding {len(valid_answers)} answers...\")\n",
    "answer_embeddings = batch_embed_texts(valid_answers, batch_size=batch_size)\n",
    "print(f\"[SUCCESS] Generated embeddings: {answer_embeddings.shape}\")\n",
    "\n",
    "# ==================== STORE IN CHROMADB (FIXED WAY) ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STORING Q&A PAIRS IN CHROMADB (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[FIX] KEY FIX: Answers will be indexed by QUESTION embeddings!\")\n",
    "print(\"   This ensures when you query with a question, you get the paired answer.\\n\")\n",
    "\n",
    "answer_ids = []\n",
    "answer_documents = []\n",
    "answer_metadatas = []\n",
    "answer_embeddings_list = []\n",
    "\n",
    "question_ids = []\n",
    "question_documents = []\n",
    "question_metadatas = []\n",
    "question_embeddings_list = []\n",
    "\n",
    "for i, pair in enumerate(valid_pairs):\n",
    "    row_idx = pair['row_index']\n",
    "    \n",
    "    # For ANSWERS: Store answer TEXT with QUESTION embedding\n",
    "    answer_ids.append(f\"answer_{row_idx}\")\n",
    "    answer_documents.append(pair['answer'])\n",
    "    answer_metadatas.append({\n",
    "        \"row_index\": int(row_idx),\n",
    "        \"question\": pair['question'][:200],  # Store question snippet for reference\n",
    "        \"answer_length\": len(pair['answer'].split())\n",
    "    })\n",
    "    # CRITICAL: Use QUESTION embedding to index the ANSWER\n",
    "    answer_embeddings_list.append(question_embeddings[i].tolist())\n",
    "    \n",
    "    # For QUESTIONS: Store question TEXT with QUESTION embedding\n",
    "    question_ids.append(f\"question_{row_idx}\")\n",
    "    question_documents.append(pair['question'])\n",
    "    question_metadatas.append({\n",
    "        \"row_index\": int(row_idx),\n",
    "        \"answer_snippet\": pair['answer'][:200],\n",
    "        \"question_length\": len(pair['question'].split())\n",
    "    })\n",
    "    question_embeddings_list.append(question_embeddings[i].tolist())\n",
    "\n",
    "# Add to ChromaDB in batches\n",
    "batch_size_db = 1000\n",
    "\n",
    "print(f\"\\nAdding {len(answer_ids)} answers (indexed by question embeddings)...\")\n",
    "for i in tqdm(range(0, len(answer_ids), batch_size_db), desc=\"Answer Batches\"):\n",
    "    batch_ids = answer_ids[i:i+batch_size_db]\n",
    "    batch_docs = answer_documents[i:i+batch_size_db]\n",
    "    batch_meta = answer_metadatas[i:i+batch_size_db]\n",
    "    batch_embs = answer_embeddings_list[i:i+batch_size_db]\n",
    "    \n",
    "    try:\n",
    "        answers_collection.add(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_meta,\n",
    "            embeddings=batch_embs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[WARNING] Error adding batch: {e}\")\n",
    "\n",
    "print(f\"[SUCCESS] Stored {len(answer_ids)} answers\")\n",
    "\n",
    "print(f\"\\nAdding {len(question_ids)} questions...\")\n",
    "for i in tqdm(range(0, len(question_ids), batch_size_db), desc=\"Question Batches\"):\n",
    "    batch_ids = question_ids[i:i+batch_size_db]\n",
    "    batch_docs = question_documents[i:i+batch_size_db]\n",
    "    batch_meta = question_metadatas[i:i+batch_size_db]\n",
    "    batch_embs = question_embeddings_list[i:i+batch_size_db]\n",
    "    \n",
    "    try:\n",
    "        questions_collection.add(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_meta,\n",
    "            embeddings=batch_embs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[WARNING] Error adding batch: {e}\")\n",
    "\n",
    "print(f\"[SUCCESS] Stored {len(question_ids)} questions\")\n",
    "\n",
    "# ==================== VERIFICATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTesting Q&A matching with first 3 pairs...\")\n",
    "\n",
    "for test_idx in range(min(3, len(valid_pairs))):\n",
    "    pair = valid_pairs[test_idx]\n",
    "    row_idx = pair['row_index']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Test {test_idx + 1}: Row {row_idx}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    print(f\"Question: {pair['question'][:80]}...\")\n",
    "    print(f\"Answer: {pair['answer'][:80]}...\")\n",
    "    \n",
    "    # Query with question embedding\n",
    "    test_q_embedding = question_embeddings[test_idx]\n",
    "    results = answers_collection.query(\n",
    "        query_embeddings=[test_q_embedding.tolist()],\n",
    "        n_results=1,\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    if results['ids'][0]:\n",
    "        matched_row = results['metadatas'][0][0].get('row_index', -1)\n",
    "        distance = results['distances'][0][0]\n",
    "        similarity = 1 - (distance ** 2) / 2\n",
    "        \n",
    "        print(f\"\\nMatched row_index: {matched_row}\")\n",
    "        print(f\"Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        if matched_row == row_idx:\n",
    "            print(\"[PASS] CORRECT! Matched to same row\")\n",
    "        else:\n",
    "            print(f\"[FAIL] WRONG! Expected row {row_idx}, got {matched_row}\")\n",
    "    else:\n",
    "        print(\"[FAIL] No results found!\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n[STATS] Processing Complete:\")\n",
    "print(f\"   Valid Q&A pairs processed: {len(valid_pairs)}\")\n",
    "print(f\"   Answers stored: {len(answer_ids)}\")\n",
    "print(f\"   Questions stored: {len(question_ids)}\")\n",
    "\n",
    "print(f\"\\n[STATS] ChromaDB Collections:\")\n",
    "print(f\"   Answers collection: {answers_collection.count()} documents\")\n",
    "print(f\"   Questions collection: {questions_collection.count()} documents\")\n",
    "\n",
    "print(f\"\\n[STORAGE] Database location: {vector_db_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[FIX] KEY FIX APPLIED:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "BEFORE (Wrong):\n",
    "  - Answers indexed by answer embeddings\n",
    "  - Query with question -> finds semantically similar answer (WRONG pair!)\n",
    "\n",
    "AFTER (Fixed):\n",
    "  - Answers indexed by QUESTION embeddings\n",
    "  - Query with question -> finds the PAIRED answer (CORRECT!)\n",
    "\n",
    "This ensures Q&A pairs stay matched correctly!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"USAGE INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Now when you query:\n",
    "\n",
    "1. Get question embedding:\n",
    "   q_embedding = get_embedding(question_text)\n",
    "\n",
    "2. Query answers collection:\n",
    "   results = answers_collection.query(\n",
    "       query_embeddings=[q_embedding],\n",
    "       n_results=1\n",
    "   )\n",
    "   \n",
    "3. You'll get the PAIRED answer for that question!\n",
    "\n",
    "Note: You can also use the row_index fallback method in your quiz\n",
    "for guaranteed correct matching.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[COMPLETE] PROCESS FINISHED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3339e083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING CHROMADB DATABASE\n",
      "======================================================================\n",
      "\n",
      "Connecting to ChromaDB...\n",
      "‚úì Connected to: vector_db\n",
      "\n",
      "üìä Collection Status:\n",
      "   Answers: 808 documents\n",
      "   Questions: 808 documents\n",
      "\n",
      "======================================================================\n",
      "PERFORMING TEST QUERY\n",
      "======================================================================\n",
      "\n",
      "Test Question: heisenberg uncertainty principle...\n",
      "Embedding shape: 768 dimensions\n",
      "\n",
      "Searching for similar answers...\n",
      "\n",
      "Top 5 most similar answers:\n",
      "\n",
      "1. Similarity: 85.93%\n",
      "   Answer: fermion type elementary particle electron quark make ordinary matter obey pauli exclusion principle state two fermion occupy quantum state time...\n",
      "   Row Index: 633\n",
      "   Question: fermion...\n",
      "\n",
      "2. Similarity: 85.37%\n",
      "   Answer: quantum mechanical principle state two identical fermion like electron occupy quantum state within system principle prevents atoms collapse give matte...\n",
      "   Row Index: 635\n",
      "   Question: pauli exclusion principle...\n",
      "\n",
      "3. Similarity: 83.09%\n",
      "   Answer: boson type elementary particle photon gluon act force carrier mediate fundamental force unlike fermion multiple boson occupy exact quantum state simul...\n",
      "   Row Index: 634\n",
      "   Question: boson...\n",
      "\n",
      "4. Similarity: 81.39%\n",
      "   Answer: lepton type elementary particle interact via strong nuclear force wellknown lepton electron example include muon tau three type neutrino...\n",
      "   Row Index: 499\n",
      "   Question: lepton...\n",
      "\n",
      "5. Similarity: 80.68%\n",
      "   Answer: fundamental theorem geometry state rightangled triangle square hypotenuse b c equal sum square two side theorem crucial calculating distance navigatio...\n",
      "   Row Index: 458\n",
      "   Question: pythagorean theorem...\n",
      "\n",
      "‚úì Test query successful!\n",
      "\n",
      "======================================================================\n",
      "TESTING QUESTION SIMILARITY\n",
      "======================================================================\n",
      "\n",
      "Finding questions similar to: heisenberg uncertainty principle...\n",
      "\n",
      "Top 5 similar questions:\n",
      "\n",
      "1. Similarity: 100.00%\n",
      "   Question: heisenberg uncertainty principle...\n",
      "   Row Index: 0\n",
      "\n",
      "2. Similarity: 100.00%\n",
      "   Question: heisenberg uncertainty principle...\n",
      "   Row Index: 559\n",
      "\n",
      "3. Similarity: 94.53%\n",
      "   Question: quantum superposition...\n",
      "   Row Index: 557\n",
      "\n",
      "4. Similarity: 93.25%\n",
      "   Question: zeroth law thermodynamics...\n",
      "   Row Index: 747\n",
      "\n",
      "5. Similarity: 92.46%\n",
      "   Question: bernoulli principle...\n",
      "   Row Index: 468\n",
      "\n",
      "‚úì Question similarity test successful!\n",
      "\n",
      "======================================================================\n",
      "SAMPLE DOCUMENTS\n",
      "======================================================================\n",
      "\n",
      "Sample Questions:\n",
      "\n",
      "1. heisenberg uncertainty principle...\n",
      "   Row: 0\n",
      "\n",
      "2. chemical formula rustfeo iron oxide primary function mitochondrion cell...\n",
      "   Row: 1\n",
      "\n",
      "3. name galaxy solar system locate...\n",
      "   Row: 2\n",
      "\n",
      "4. cause ocean tide...\n",
      "   Row: 3\n",
      "\n",
      "5. catalyst...\n",
      "   Row: 4\n",
      "\n",
      "\n",
      "Sample Answers:\n",
      "\n",
      "1. state position momentum particle precisely know simultaneously accurately one know less accurately...\n",
      "   Row: 0\n",
      "   Question: heisenberg uncertainty principle...\n",
      "\n",
      "2. generate energy atp cellular respiration often call powerhouse cell...\n",
      "   Row: 1\n",
      "   Question: chemical formula rustfeo iron oxide primary function mitochondrion cell...\n",
      "\n",
      "3. milky way galaxy...\n",
      "   Row: 2\n",
      "   Question: name galaxy solar system locate...\n",
      "\n",
      "4. primarily gravitational pull moon lesser extent sun earth ocean...\n",
      "   Row: 3\n",
      "   Question: cause ocean tide...\n",
      "\n",
      "5. substance speed chemical reaction without consume reaction lower activation energy require reaction ...\n",
      "   Row: 4\n",
      "   Question: catalyst...\n",
      "\n",
      "======================================================================\n",
      "‚úì‚úì‚úì TEST COMPLETE ‚úì‚úì‚úì\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "vector_db_dir = \"vector_db\"\n",
    "\n",
    "# ==================== CONNECT TO CHROMADB ====================\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING CHROMADB DATABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nConnecting to ChromaDB...\")\n",
    "print(f\"‚úì Connected to: {vector_db_dir}\")\n",
    "\n",
    "# Load collections\n",
    "answers_collection = client.get_collection(\"science_qa_answers\")\n",
    "questions_collection = client.get_collection(\"science_qa_questions\")\n",
    "\n",
    "print(f\"\\nüìä Collection Status:\")\n",
    "print(f\"   Answers: {answers_collection.count()} documents\")\n",
    "print(f\"   Questions: {questions_collection.count()} documents\")\n",
    "\n",
    "# ==================== TEST QUERY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMING TEST QUERY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if answers_collection.count() > 0 and questions_collection.count() > 0:\n",
    "    # Get first question with its embedding\n",
    "    first_question_data = questions_collection.get(\n",
    "        limit=1,\n",
    "        include=['embeddings', 'documents', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    # Check if we got data\n",
    "    if first_question_data and first_question_data.get('documents') and len(first_question_data['documents']) > 0:\n",
    "        test_question = first_question_data['documents'][0]\n",
    "        test_embedding = first_question_data['embeddings'][0]\n",
    "        \n",
    "        print(f\"\\nTest Question: {test_question[:150]}...\")\n",
    "        print(f\"Embedding shape: {len(test_embedding)} dimensions\")\n",
    "        \n",
    "        # Query for similar answers\n",
    "        print(\"\\nSearching for similar answers...\")\n",
    "        results = answers_collection.query(\n",
    "            query_embeddings=[test_embedding],\n",
    "            n_results=5,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTop 5 most similar answers:\\n\")\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            doc = results['documents'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            distance = results['distances'][0][i]\n",
    "            \n",
    "            # Calculate similarity (cosine similarity from L2 distance)\n",
    "            similarity = 1 - (distance ** 2) / 2\n",
    "            \n",
    "            print(f\"{i+1}. Similarity: {similarity:.2%}\")\n",
    "            print(f\"   Answer: {doc[:150]}...\")\n",
    "            print(f\"   Row Index: {metadata.get('row_index', 'N/A')}\")\n",
    "            print(f\"   Question: {metadata.get('question', 'N/A')[:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(\"‚úì Test query successful!\")\n",
    "        \n",
    "        # ==================== TEST QUESTION-TO-QUESTION SIMILARITY ====================\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TESTING QUESTION SIMILARITY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nFinding questions similar to: {test_question[:100]}...\")\n",
    "        \n",
    "        similar_questions = questions_collection.query(\n",
    "            query_embeddings=[test_embedding],\n",
    "            n_results=5,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTop 5 similar questions:\\n\")\n",
    "        for i in range(len(similar_questions['documents'][0])):\n",
    "            doc = similar_questions['documents'][0][i]\n",
    "            metadata = similar_questions['metadatas'][0][i]\n",
    "            distance = similar_questions['distances'][0][i]\n",
    "            similarity = 1 - (distance ** 2) / 2\n",
    "            \n",
    "            print(f\"{i+1}. Similarity: {similarity:.2%}\")\n",
    "            print(f\"   Question: {doc[:150]}...\")\n",
    "            print(f\"   Row Index: {metadata.get('row_index', 'N/A')}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"‚úì Question similarity test successful!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö† No data found in questions collection\")\n",
    "else:\n",
    "    print(\"‚ö† Collections are empty, cannot perform test\")\n",
    "\n",
    "# ==================== SAMPLE DOCUMENTS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DOCUMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nSample Questions:\")\n",
    "sample_questions = questions_collection.get(\n",
    "    limit=5,\n",
    "    include=['documents', 'metadatas']\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(sample_questions['documents'], sample_questions['metadatas'])):\n",
    "    print(f\"\\n{i+1}. {doc[:100]}...\")\n",
    "    print(f\"   Row: {meta.get('row_index', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\\nSample Answers:\")\n",
    "sample_answers = answers_collection.get(\n",
    "    limit=5,\n",
    "    include=['documents', 'metadatas']\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(sample_answers['documents'], sample_answers['metadatas'])):\n",
    "    print(f\"\\n{i+1}. {doc[:100]}...\")\n",
    "    print(f\"   Row: {meta.get('row_index', 'N/A')}\")\n",
    "    print(f\"   Question: {meta.get('question', 'N/A')[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì‚úì‚úì TEST COMPLETE ‚úì‚úì‚úì\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0970722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHROMADB DIAGNOSTIC & FIX\n",
      "======================================================================\n",
      "\n",
      "‚úì Database directory exists: vector_db\n",
      "\n",
      "Attempting to connect to ChromaDB...\n",
      "‚úì Connected successfully\n",
      "\n",
      "======================================================================\n",
      "CHECKING COLLECTIONS\n",
      "======================================================================\n",
      "\n",
      "Found 4 collections:\n",
      "  - science_qa_questions: 980 documents\n",
      "  - qa_answers: 12354 documents\n",
      "  - qa_questions: 12354 documents\n",
      "  - science_qa_answers: 980 documents\n",
      "\n",
      "======================================================================\n",
      "TESTING COLLECTION ACCESS\n",
      "======================================================================\n",
      "\n",
      "All collections: ['science_qa_questions', 'qa_answers', 'qa_questions', 'science_qa_answers']\n",
      "Required collections: ['science_qa_answers', 'science_qa_questions']\n",
      "\n",
      "======================================================================\n",
      "Testing: science_qa_questions\n",
      "======================================================================\n",
      "  ‚úì Collection exists: 980 documents\n",
      "  ‚úì Can read documents\n",
      "    Sample doc: heisenberg uncertainty principle...\n",
      "  ‚úì Can read embeddings: dimension 768\n",
      "  ‚Üí Testing query...\n",
      "  ‚úì Query works! Got 3 results\n",
      "    Top result distance: 0.0000\n",
      "  ‚úì Metadata: {'row_index': 0, 'question_length': 3, 'answer': 'state position momentum particle precisely know simultaneously accurately one know less accurately'}\n",
      "\n",
      "======================================================================\n",
      "Testing: qa_answers\n",
      "======================================================================\n",
      "  ‚úì Collection exists: 12354 documents\n",
      "  ‚úì Can read documents\n",
      "    Sample doc: virus consist solely nucleic acid protein lacking metabolic reaction requiring host selfreplication...\n",
      "  ‚úì Can read embeddings: dimension 768\n",
      "  ‚Üí Testing query...\n",
      "  ‚úì Query works! Got 3 results\n",
      "    Top result distance: 0.0000\n",
      "  ‚úì Metadata: {'question_column': 'question1', 'row_index': 0, 'answer_column': 'answer1', 'question': 'virus different microbe', 'pair_index': 1, 'answer_length': 12}\n",
      "\n",
      "======================================================================\n",
      "Testing: qa_questions\n",
      "======================================================================\n",
      "  ‚úì Collection exists: 12354 documents\n",
      "  ‚úì Can read documents\n",
      "    Sample doc: virus different microbe...\n",
      "  ‚úì Can read embeddings: dimension 768\n",
      "  ‚Üí Testing query...\n",
      "  ‚úì Query works! Got 3 results\n",
      "    Top result distance: 0.0000\n",
      "  ‚úì Metadata: {'pair_index': 1, 'answer_column': 'answer1', 'row_index': 0, 'answer': 'virus consist solely nucleic acid protein lacking metabolic reaction requiring host selfreplication', 'question_column': 'question1'}\n",
      "\n",
      "======================================================================\n",
      "Testing: science_qa_answers\n",
      "======================================================================\n",
      "  ‚úì Collection exists: 980 documents\n",
      "  ‚úì Can read documents\n",
      "    Sample doc: state position momentum particle precisely know simultaneously accurately one know less accurately...\n",
      "  ‚úì Can read embeddings: dimension 768\n",
      "  ‚Üí Testing query...\n",
      "  ‚úì Query works! Got 3 results\n",
      "    Top result distance: 0.0000\n",
      "  ‚úì Metadata: {'answer_length': 12, 'question': 'heisenberg uncertainty principle', 'row_index': 0}\n",
      "\n",
      "======================================================================\n",
      "SUMMARY & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "üìä Collection Status:\n",
      "  ‚úÖ science_qa_questions: OK\n",
      "  ‚úÖ qa_answers: OK\n",
      "  ‚úÖ qa_questions: OK\n",
      "  ‚úÖ science_qa_answers: OK\n",
      "\n",
      "‚úÖ Working collections: ['science_qa_questions', 'qa_answers', 'qa_questions', 'science_qa_answers']\n",
      "   You can use these collections in your application!\n",
      "\n",
      "‚ú® GREAT NEWS: Both required collections are working!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "vector_db_dir = \"vector_db\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHROMADB DIAGNOSTIC & FIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Check if database exists\n",
    "if not os.path.exists(vector_db_dir):\n",
    "    print(f\"\\n‚ùå Database directory does not exist: {vector_db_dir}\")\n",
    "    print(\"You need to run the embedding generation script first!\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\n‚úì Database directory exists: {vector_db_dir}\")\n",
    "\n",
    "# Step 2: Try to connect\n",
    "print(\"\\nAttempting to connect to ChromaDB...\")\n",
    "try:\n",
    "    client = chromadb.PersistentClient(path=vector_db_dir)\n",
    "    print(\"‚úì Connected successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nTrying to fix...\")\n",
    "    \n",
    "    # Remove lock files\n",
    "    import glob\n",
    "    lock_files = glob.glob(os.path.join(vector_db_dir, \"**/*.lock\"), recursive=True)\n",
    "    for lock_file in lock_files:\n",
    "        try:\n",
    "            os.remove(lock_file)\n",
    "            print(f\"  Removed lock: {lock_file}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try again\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=vector_db_dir)\n",
    "        print(\"‚úì Connected after cleanup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failed: {e2}\")\n",
    "        print(\"\\nRecommendation: Delete vector_db folder and re-run embedding script\")\n",
    "        exit(1)\n",
    "\n",
    "# Step 3: Check collections\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING COLLECTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    collections = client.list_collections()\n",
    "    print(f\"\\nFound {len(collections)} collections:\")\n",
    "    for coll in collections:\n",
    "        print(f\"  - {coll.name}: {coll.count()} documents\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error listing collections: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 4: Try to access specific collections\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING COLLECTION ACCESS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test all collections\n",
    "all_collection_names = [coll.name for coll in collections]\n",
    "required_collections = [\"science_qa_answers\", \"science_qa_questions\"]\n",
    "collection_status = {}\n",
    "\n",
    "print(f\"\\nAll collections: {all_collection_names}\")\n",
    "print(f\"Required collections: {required_collections}\")\n",
    "\n",
    "for coll_name in all_collection_names:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {coll_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    try:\n",
    "        coll = client.get_collection(coll_name)\n",
    "        count = coll.count()\n",
    "        print(f\"  ‚úì Collection exists: {count} documents\")\n",
    "        \n",
    "        # Try to get a sample\n",
    "        try:\n",
    "            sample = coll.get(limit=1, include=['documents', 'embeddings', 'metadatas'])\n",
    "            if sample['documents']:\n",
    "                print(f\"  ‚úì Can read documents\")\n",
    "                print(f\"    Sample doc: {sample['documents'][0][:100]}...\")\n",
    "                \n",
    "                # FIXED: Check embeddings properly\n",
    "                if sample['embeddings'] is not None and len(sample['embeddings']) > 0:\n",
    "                    print(f\"  ‚úì Can read embeddings: dimension {len(sample['embeddings'][0])}\")\n",
    "                    \n",
    "                    # Try a test query\n",
    "                    print(f\"  ‚Üí Testing query...\")\n",
    "                    try:\n",
    "                        test_results = coll.query(\n",
    "                            query_embeddings=[sample['embeddings'][0]],\n",
    "                            n_results=3,\n",
    "                            include=['documents', 'distances']\n",
    "                        )\n",
    "                        print(f\"  ‚úì Query works! Got {len(test_results['documents'][0])} results\")\n",
    "                        print(f\"    Top result distance: {test_results['distances'][0][0]:.4f}\")\n",
    "                        collection_status[coll_name] = \"OK\"\n",
    "                    except Exception as qe:\n",
    "                        print(f\"  ‚ùå Query failed: {qe}\")\n",
    "                        collection_status[coll_name] = f\"QUERY_ERROR: {str(qe)[:100]}\"\n",
    "                else:\n",
    "                    print(f\"  ‚ùå No embeddings found!\")\n",
    "                    collection_status[coll_name] = \"NO_EMBEDDINGS\"\n",
    "                    \n",
    "                if sample['metadatas']:\n",
    "                    print(f\"  ‚úì Metadata: {sample['metadatas'][0]}\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Collection is empty!\")\n",
    "                collection_status[coll_name] = \"EMPTY\"\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error reading data: {e}\")\n",
    "            import traceback\n",
    "            print(f\"  Traceback: {traceback.format_exc()}\")\n",
    "            collection_status[coll_name] = f\"READ_ERROR: {str(e)[:100]}\"\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Collection access failed: {e}\")\n",
    "        collection_status[coll_name] = f\"ACCESS_ERROR: {str(e)[:100]}\"\n",
    "\n",
    "# Step 5: Summary and recommendations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Collection Status:\")\n",
    "for coll, status in collection_status.items():\n",
    "    status_emoji = \"‚úÖ\" if status == \"OK\" else \"‚ùå\"\n",
    "    print(f\"  {status_emoji} {coll}: {status}\")\n",
    "\n",
    "# Check which collections work\n",
    "working_collections = [coll for coll, status in collection_status.items() if status == \"OK\"]\n",
    "broken_collections = [coll for coll, status in collection_status.items() if status != \"OK\"]\n",
    "\n",
    "if working_collections:\n",
    "    print(f\"\\n‚úÖ Working collections: {working_collections}\")\n",
    "    print(f\"   You can use these collections in your application!\")\n",
    "    \n",
    "    if \"science_qa_answers\" in working_collections and \"science_qa_questions\" in working_collections:\n",
    "        print(\"\\n‚ú® GREAT NEWS: Both required collections are working!\")\n",
    "    elif \"qa_answers\" in working_collections and \"qa_questions\" in working_collections:\n",
    "        print(\"\\nüí° TIP: The 'qa_answers' and 'qa_questions' collections are working.\")\n",
    "        print(\"   Consider using these instead of the science_qa_* versions.\")\n",
    "\n",
    "if broken_collections:\n",
    "    print(f\"\\n‚ùå Broken collections: {broken_collections}\")\n",
    "    print(\"\\nüîß Recommended fixes:\")\n",
    "    \n",
    "    for coll in broken_collections:\n",
    "        status = collection_status[coll]\n",
    "        print(f\"\\n  {coll}:\")\n",
    "        if \"QUERY_ERROR\" in status:\n",
    "            if \"Nothing found on disk\" in status:\n",
    "                print(f\"    - Embeddings not properly saved to disk\")\n",
    "                print(f\"    - Delete collection and regenerate:\")\n",
    "                print(f\"      client.delete_collection('{coll}')\")\n",
    "                print(f\"      Then re-run embedding script\")\n",
    "            else:\n",
    "                print(f\"    - Query operation failed\")\n",
    "                print(f\"    - May need to regenerate embeddings\")\n",
    "        elif \"NO_EMBEDDINGS\" in status:\n",
    "            print(f\"    - Embeddings missing from collection\")\n",
    "            print(f\"    - Re-run embedding generation script\")\n",
    "        elif \"EMPTY\" in status:\n",
    "            print(f\"    - Collection has no data\")\n",
    "            print(f\"    - Re-run embedding script\")\n",
    "        else:\n",
    "            print(f\"    - Unknown issue: {status}\")\n",
    "            print(f\"    - Try deleting and regenerating collection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73c4ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHROMADB Q&A MATCHING DIAGNOSTIC\n",
      "======================================================================\n",
      "\n",
      "‚úì Loaded original CSV: 980 rows\n",
      "‚úì Loaded preprocessed CSV: 980 rows\n",
      "\n",
      "‚úì Answers collection: 980 documents\n",
      "‚úì Questions collection: 980 documents\n",
      "\n",
      "======================================================================\n",
      "TESTING QUESTION-ANSWER MATCHING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST ROW 0\n",
      "======================================================================\n",
      "\n",
      "üìù Original Question:\n",
      "   What is the Heisenberg Uncertainty Principle?...\n",
      "\n",
      "üìù Original Answer:\n",
      "   It states that the position and momentum of a particle cannot both be precisely known simultaneously...\n",
      "\n",
      "üîß Preprocessed Question:\n",
      "   heisenberg uncertainty principle...\n",
      "\n",
      "üîß Preprocessed Answer:\n",
      "   state position momentum particle precisely know simultaneously accurately one know less accurately...\n",
      "\n",
      "‚úì Found question in DB (position 0):\n",
      "   heisenberg uncertainty principle...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚úì Found answer in DB (position 0):\n",
      "   state position momentum particle precisely know simultaneously accurately one know less accurately...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TESTING QUERY MECHANISM\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ùå Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "TEST ROW 1\n",
      "======================================================================\n",
      "\n",
      "üìù Original Question:\n",
      "   What is the chemical formula for rust?\",Fe‚ÇÇO‚ÇÉ (Iron Oxide).\n",
      "What is the primary function of mitocho...\n",
      "\n",
      "üìù Original Answer:\n",
      "   To generate energy (ATP) through cellular respiration. They are often called the \"powerhouse\" of the...\n",
      "\n",
      "üîß Preprocessed Question:\n",
      "   chemical formula rustfeo iron oxide primary function mitochondrion cell...\n",
      "\n",
      "üîß Preprocessed Answer:\n",
      "   generate energy atp cellular respiration often call powerhouse cell...\n",
      "\n",
      "‚úì Found question in DB (position 1):\n",
      "   chemical formula rustfeo iron oxide primary function mitochondrion cell...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚úì Found answer in DB (position 1):\n",
      "   generate energy atp cellular respiration often call powerhouse cell...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TESTING QUERY MECHANISM\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ùå Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "TEST ROW 2\n",
      "======================================================================\n",
      "\n",
      "üìù Original Question:\n",
      "   What is the name of the galaxy our solar system is located in?...\n",
      "\n",
      "üìù Original Answer:\n",
      "   The Milky Way galaxy....\n",
      "\n",
      "üîß Preprocessed Question:\n",
      "   name galaxy solar system locate...\n",
      "\n",
      "üîß Preprocessed Answer:\n",
      "   milky way galaxy...\n",
      "\n",
      "‚úì Found question in DB (position 2):\n",
      "   name galaxy solar system locate...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚úì Found answer in DB (position 2):\n",
      "   milky way galaxy...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TESTING QUERY MECHANISM\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ùå Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "TEST ROW 5\n",
      "======================================================================\n",
      "\n",
      "üìù Original Question:\n",
      "   What is the primary function of ribosomes?...\n",
      "\n",
      "üìù Original Answer:\n",
      "   Ribosomes are the protein synthesis machinery of the cell. They translate mRNA into proteins....\n",
      "\n",
      "üîß Preprocessed Question:\n",
      "   primary function ribosome...\n",
      "\n",
      "üîß Preprocessed Answer:\n",
      "   ribosome protein synthesis machinery cell translate mrna protein...\n",
      "\n",
      "‚úì Found question in DB (position 5):\n",
      "   primary function ribosome...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚úì Found answer in DB (position 5):\n",
      "   ribosome protein synthesis machinery cell translate mrna protein...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TESTING QUERY MECHANISM\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ùå Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "TEST ROW 10\n",
      "======================================================================\n",
      "\n",
      "üìù Original Question:\n",
      "   What is DNA?...\n",
      "\n",
      "üìù Original Answer:\n",
      "   DNA, or deoxyribonucleic acid, is the molecule that carries the genetic instructions for the develop...\n",
      "\n",
      "üîß Preprocessed Question:\n",
      "   dna...\n",
      "\n",
      "üîß Preprocessed Answer:\n",
      "   dna deoxyribonucleic acid molecule carry genetic instruction development function reproduction know ...\n",
      "\n",
      "‚úì Found question in DB (position 10):\n",
      "   dna...\n",
      "\n",
      "   Text match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akqp4\\AppData\\Local\\Temp\\ipykernel_42228\\900107719.py\", line 131, in <module>\n",
      "    if q_results['embeddings']:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akqp4\\AppData\\Local\\Temp\\ipykernel_42228\\900107719.py\", line 131, in <module>\n",
      "    if q_results['embeddings']:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akqp4\\AppData\\Local\\Temp\\ipykernel_42228\\900107719.py\", line 131, in <module>\n",
      "    if q_results['embeddings']:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akqp4\\AppData\\Local\\Temp\\ipykernel_42228\\900107719.py\", line 131, in <module>\n",
      "    if q_results['embeddings']:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akqp4\\AppData\\Local\\Temp\\ipykernel_42228\\900107719.py\", line 131, in <module>\n",
      "    if q_results['embeddings']:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Found answer in DB (position 10):\n",
      "   dna deoxyribonucleic acid molecule carry genetic instruction development function reproduction know ...\n",
      "\n",
      "   Text match: True\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "TESTING QUERY MECHANISM\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ùå Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "CHECKING EMBEDDING STORAGE\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Get first 10 items from each collection\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m q_sample \u001b[38;5;241m=\u001b[39m \u001b[43mquestions_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadatas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m a_sample \u001b[38;5;241m=\u001b[39m answers_collection\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    178\u001b[0m     limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    179\u001b[0m     include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    180\u001b[0m )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFirst 10 Questions in DB:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:128\u001b[0m, in \u001b[0;36mCollection.get\u001b[1;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    106\u001b[0m     ids: Optional[OneOrMany[ID]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    112\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GetResult:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings and their associate data from the data store. If no ids or where filter is provided returns\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    all embeddings up to limit starting at offset.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     get_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_get_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     get_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get(\n\u001b[0;32m    136\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    137\u001b[0m         ids\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_get_response(\n\u001b[0;32m    147\u001b[0m         response\u001b[38;5;241m=\u001b[39mget_results, include\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    148\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:103\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    105\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:274\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_get_request\u001b[1;34m(self, ids, where, where_document, include)\u001b[0m\n\u001b[0;32m    271\u001b[0m     validate_ids(ids\u001b[38;5;241m=\u001b[39munpacked_ids)\n\u001b[0;32m    273\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[1;32m--> 274\u001b[0m \u001b[43mvalidate_include\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdissalowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set a data loader on the collection if loading from URIs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\chromadb\\api\\types.py:1239\u001b[0m, in \u001b[0;36mvalidate_include\u001b[1;34m(include, dissalowed)\u001b[0m\n\u001b[0;32m   1237\u001b[0m valid_items \u001b[38;5;241m=\u001b[39m get_args(get_args(Include)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_items:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(valid_items)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInclude item cannot be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(dissalowed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1246\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get."
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "vector_db_dir = \"vector_db\"\n",
    "original_csv = \"science_qa.csv\"\n",
    "preprocessed_csv = \"preprocessed_science_qa.csv\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHROMADB Q&A MATCHING DIAGNOSTIC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load CSVs\n",
    "df_original = pd.read_csv(original_csv)\n",
    "df_preprocessed = pd.read_csv(preprocessed_csv)\n",
    "\n",
    "print(f\"\\n‚úì Loaded original CSV: {len(df_original)} rows\")\n",
    "print(f\"‚úì Loaded preprocessed CSV: {len(df_preprocessed)} rows\")\n",
    "\n",
    "# Find columns\n",
    "def find_column(df, name):\n",
    "    for col in df.columns:\n",
    "        if col.lower() == name.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "question_col = find_column(df_original, 'question')\n",
    "answer_col = find_column(df_original, 'answer')\n",
    "question_col_prep = find_column(df_preprocessed, 'question')\n",
    "answer_col_prep = find_column(df_preprocessed, 'answer')\n",
    "\n",
    "# Connect to ChromaDB\n",
    "client = chromadb.PersistentClient(path=vector_db_dir)\n",
    "answers_collection = client.get_collection(\"science_qa_answers\")\n",
    "questions_collection = client.get_collection(\"science_qa_questions\")\n",
    "\n",
    "print(f\"\\n‚úì Answers collection: {answers_collection.count()} documents\")\n",
    "print(f\"‚úì Questions collection: {questions_collection.count()} documents\")\n",
    "\n",
    "# ==================== TEST SPECIFIC EXAMPLES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING QUESTION-ANSWER MATCHING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with first few rows\n",
    "test_indices = [0, 1, 2, 5, 10]\n",
    "\n",
    "for idx in test_indices:\n",
    "    if idx >= len(df_original):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST ROW {idx}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get original texts\n",
    "    orig_question = df_original[question_col].iloc[idx]\n",
    "    orig_answer = df_original[answer_col].iloc[idx]\n",
    "    \n",
    "    print(f\"\\nüìù Original Question:\")\n",
    "    print(f\"   {orig_question[:100]}...\")\n",
    "    print(f\"\\nüìù Original Answer:\")\n",
    "    print(f\"   {orig_answer[:100]}...\")\n",
    "    \n",
    "    # Get preprocessed texts\n",
    "    prep_question = df_preprocessed[question_col_prep].iloc[idx]\n",
    "    prep_answer = df_preprocessed[answer_col_prep].iloc[idx]\n",
    "    \n",
    "    print(f\"\\nüîß Preprocessed Question:\")\n",
    "    print(f\"   {prep_question[:100]}...\")\n",
    "    print(f\"\\nüîß Preprocessed Answer:\")\n",
    "    print(f\"   {prep_answer[:100]}...\")\n",
    "    \n",
    "    # Try to find question in DB\n",
    "    try:\n",
    "        # Get all questions and check if our row_idx exists\n",
    "        all_questions = questions_collection.get(\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        # Find matching metadata\n",
    "        matching_q_idx = None\n",
    "        for i, meta in enumerate(all_questions['metadatas']):\n",
    "            if meta and 'row_index' in meta and meta['row_index'] == idx:\n",
    "                matching_q_idx = i\n",
    "                break\n",
    "        \n",
    "        if matching_q_idx is not None:\n",
    "            db_question = all_questions['documents'][matching_q_idx]\n",
    "            print(f\"\\n‚úì Found question in DB (position {matching_q_idx}):\")\n",
    "            print(f\"   {db_question[:100]}...\")\n",
    "            print(f\"\\n   Text match: {db_question == prep_question}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Question with row_index={idx} NOT FOUND in DB!\")\n",
    "            continue\n",
    "        \n",
    "        # Get all answers and check if our row_idx exists\n",
    "        all_answers = answers_collection.get(\n",
    "            include=['documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        # Find matching metadata\n",
    "        matching_a_idx = None\n",
    "        for i, meta in enumerate(all_answers['metadatas']):\n",
    "            if meta and 'row_index' in meta and meta['row_index'] == idx:\n",
    "                matching_a_idx = i\n",
    "                break\n",
    "        \n",
    "        if matching_a_idx is not None:\n",
    "            db_answer = all_answers['documents'][matching_a_idx]\n",
    "            print(f\"\\n‚úì Found answer in DB (position {matching_a_idx}):\")\n",
    "            print(f\"   {db_answer[:100]}...\")\n",
    "            print(f\"\\n   Text match: {db_answer == prep_answer}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Answer with row_index={idx} NOT FOUND in DB!\")\n",
    "            continue\n",
    "        \n",
    "        # Now test the query mechanism\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"TESTING QUERY MECHANISM\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        # Query with question embedding\n",
    "        q_results = questions_collection.get(\n",
    "            ids=[all_questions['ids'][matching_q_idx]],\n",
    "            include=['embeddings']\n",
    "        )\n",
    "        \n",
    "        if q_results['embeddings']:\n",
    "            question_embedding = q_results['embeddings'][0]\n",
    "            \n",
    "            # Query answers collection\n",
    "            a_results = answers_collection.query(\n",
    "                query_embeddings=[question_embedding],\n",
    "                n_results=3,\n",
    "                include=['documents', 'metadatas', 'distances']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nTop 3 answer matches when querying with question embedding:\")\n",
    "            for i, (doc, meta, dist) in enumerate(zip(\n",
    "                a_results['documents'][0],\n",
    "                a_results['metadatas'][0],\n",
    "                a_results['distances'][0]\n",
    "            )):\n",
    "                similarity = 1 - (dist ** 2) / 2\n",
    "                row_idx_result = meta.get('row_index', 'N/A') if meta else 'N/A'\n",
    "                print(f\"\\n  Match {i+1}:\")\n",
    "                print(f\"    Row index: {row_idx_result}\")\n",
    "                print(f\"    Similarity: {similarity:.2%}\")\n",
    "                print(f\"    Text: {doc[:80]}...\")\n",
    "                \n",
    "                if i == 0:\n",
    "                    if row_idx_result == idx:\n",
    "                        print(f\"    ‚úÖ CORRECT! Matched to same row index\")\n",
    "                    else:\n",
    "                        print(f\"    ‚ùå WRONG! Expected row {idx}, got row {row_idx_result}\")\n",
    "                        print(f\"    üö® THIS IS THE BUG!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==================== CHECK EMBEDDING CORRELATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING EMBEDDING STORAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get first 10 items from each collection\n",
    "q_sample = questions_collection.get(\n",
    "    limit=10,\n",
    "    include=['documents', 'metadatas', 'ids']\n",
    ")\n",
    "\n",
    "a_sample = answers_collection.get(\n",
    "    limit=10,\n",
    "    include=['documents', 'metadatas', 'ids']\n",
    ")\n",
    "\n",
    "print(\"\\nFirst 10 Questions in DB:\")\n",
    "for i, (doc, meta, id_) in enumerate(zip(q_sample['documents'], q_sample['metadatas'], q_sample['ids'])):\n",
    "    row_idx = meta.get('row_index', 'N/A') if meta else 'N/A'\n",
    "    print(f\"  {i}: ID={id_}, row_index={row_idx}, text={doc[:50]}...\")\n",
    "\n",
    "print(\"\\nFirst 10 Answers in DB:\")\n",
    "for i, (doc, meta, id_) in enumerate(zip(a_sample['documents'], a_sample['metadatas'], a_sample['ids'])):\n",
    "    row_idx = meta.get('row_index', 'N/A') if meta else 'N/A'\n",
    "    print(f\"  {i}: ID={id_}, row_index={row_idx}, text={doc[:50]}...\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîç Check the following:\")\n",
    "print(\"  1. Do questions and answers have the SAME row_index in metadata?\")\n",
    "print(\"  2. When querying with a question embedding, does it return the\")\n",
    "print(\"     answer with the SAME row_index?\")\n",
    "print(\"  3. Are the IDs in sequential order or random?\")\n",
    "print(\"\\nüí° If answers don't match questions by row_index, your embedding\")\n",
    "print(\"   generation script needs to be fixed!\")\n",
    "print(\"\\nüîß The fix: Ensure when generating embeddings, you store:\")\n",
    "print(\"   - Question embedding with metadata={'row_index': i}\")\n",
    "print(\"   - Answer embedding with metadata={'row_index': i}\")\n",
    "print(\"   Where i is the SAME index from the CSV\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ee864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akqp4\\python_projects\\NLP\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "file_path = r\"Preprocessed_QA_Dataset.csv\"\n",
    "output_dir = r\"NLP\"\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 32 \n",
    "\n",
    "# Device and model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair Question1 embedding with Answer1 embedding for similarity calculation\n",
    "q1_embs = np.load(\"Question1_embeddings.npy\")\n",
    "a1_embs = np.load(\"Answer1_embeddings.npy\")\n",
    "\n",
    "# Cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "scores = cosine_similarity(q1_embs, a1_embs)  # row-wise similarity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
